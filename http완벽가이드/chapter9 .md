# 9. 웹 로봇

웹 로봇이란 사람과의 상호작용 없이 연속된 웹 트랜젝션들을 자동으로 수행하는 소프트웨어 프로그램이다. 많은 로봇들이 웹 사이트에서 다른 웹 사이트로 떠돌아다니면서, 콘텐츠를 가져오고, 하이퍼링크를 따라가고, 그들이 발견한 데이터를 처리한다.s

## 9.1 크롤러와 크롤링

HTML 하이퍼링크로 만들어진 웹 링크를 재귀적 반복하며 웹을 순회하는것을 말한다.

인터넷 검색엔진은 웹을 돌아다니면서 그들이 만나는 모든 문서를 끌어오기 위해 크롤러를 사용한다.

### 9.1.1 어디에서 시작하는가: ‘루트 집합’

크롤러가 방문을 시작하는 URL들의 초기 집합을 루트 집합(root set)이라고 한다. 루트 집합을 고를 때, 모든 링크를 크롤링하면 결과적으로 관심있는 웹 페이지들의 대부분을 가져오게 될 수 있도록 충분히 다른장소에서 URL들을 선택해야 한다.

**루트집합의 일반적인 구성**

1. 크고 인기있는 웹사이트
2. 새로 생성된 페이지들의 목록
3. 자주 링크되지 않는 잘 알려있지 않은 페이지들의 목록

### 9.1.2 링크 추출과 상대 링크 정상화

크롤러들은 간단한 HTML을 파싱해서 이들 링크들을 추출하고, 상대링크를 절대 링크로 변환해야한다.

크롤러가 크롤링을 진행하면서 탐색해야 할 새 링크를 발견함에 따라, 이 목록은 급속히 확장된다.

### 9.1.3 순환 피하기

로봇에 웹 크롤링을 할 때, 루프나 순환에 빠지지 않게 매우 조심해야하며, 순환을 피하기 위해선 반드시 어디를 방문했는지 알아야한다.

### 9.1.4 루프와 중복

**순환이 해로운 이유 3가지**

1. 순환은 크롤러를 루프에 빠져 그 어떤 페이지도 가져올 수 없게 만들 수 있다.
2. 같은 페이지를 반복적으로 가져오게 하면 웹 서버에 부담이 되고, 실 사용자도 사이트 접근이 불가하게 될 수 도 있다.
3. 많은 중복된 페이지를 가져오게 되어, 쓸모없는 중복 컨텐츠들로 넘쳐나게 된다.

### 9.1.5 빵 부스러기의 흔적

방문한 곳을 지속적으로 추적하는것은 어려운 일이며, 어떤 URL을 방문했는지 빠르게 판단하기 위해선 복잡한 자료구조를 사용해야한다. 이 자료구조는 속도와 메모리 사용 면에서 효과적이어야 한다.

- **트리와 해시테이블**
  - URL을 훨씬 빠르게 찾아볼 수 있게 해주는 소프트웨어 자료구조
- **느슨한 존재 비트맵**
  - 공간 사용을 최소화하기 위해 몇몇 대규모 크롤러들은 존재 비트 배열(presence bit array)와 같은 느슨한 자료구조를 사용한다.
  - URL이 크롤링 되었을 때, 해당하는 존재비트가 만들어지고 만약 존재비트가 이미 존재한다면 크롤러는 그 URL을 이미 크롤링 되었다고 간주한다.
- **체크포인트**
  - 로봇 프로그램이 갑작스럽게 중단될 경우를 대비해, 방문한 URL의 목록이 디스크에 저장되었는지 확인한다.
- **파티셔닝**
  - 웹이 성장하면서 한대의 컴퓨터가 크롤링을 완수하기에 무리이므로, 각각 분리된 한 대의 컴퓨터에 할당되어 커뮤니케이션 하며 크롤링을 한다.

### 9.1.6 별칭(alias)과 로봇 순환

같은 문서를 가리키는 다른 URL들의 예

| 첫번째 URL                    | 두번째 URL                      | 어떤 경우에 같은 URL을 가리키게 되는가  |
| ----------------------------- | ------------------------------- | --------------------------------------- |
| http://www.foo.com/bar.html   | http://www.foo.com:80/bar.html  | 기본 포트가 80번일 경우                 |
| http://www.foo.com/~fred      | http://www.foo.com/%7fred       | %7과 ~ 가 같을 경우                     |
| http://www.foo.com/readme.htm | http://www.foo.com/README.htm   | 서버가 대소문자 구분을 안할경우         |
| http://www.foo.com/           | http://www.foo.com/index.html   | 기본 페이지가 index.html인 경우         |
| http://www.foo.com/x.html#a   | http://www.foo.com/x.html/#b    | 태그에 따라 페이지가 변경되지 않을 경우 |
| http://www.foo.com/           | http://209.231.87.45/index.html | www.foo.com이 이 아이피 주소를 가질 떄  |

### 9.1.7 URL 정규화하기

대부분의 웹 로봇은 URL들을 표준 형식으로 ‘정규화’함으로써 다른 URL과 같은 리소스를 가지고 있는것을 미리 제거하려고 한다. 하지만 기본적인 문법들을 변환한다고 해서 또 다른 별칭들을 만날 수 있다.

1. 포트번호가 명시되지 않았다면 호스트 명에 ‘:80’ 을 추가한다.
2. 모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환한다.
3. # 태그들을 제거한다.

### 9.1.8 파일 시스템 링크 순환

파일 시스템의 심벌릭 링크는 아무것도 존재하지 않으면서 끝없이 깊어지는 디렉터리 계층을 만들 수 있다. 이는 교묘한 종류의 순환을 유발할 수 있어 로봇을 함정에 빠뜨리기 위해 악의적으로 만들기도 한다.

심벌릭 링크를 사용하여 경로를 참조하게 만들어 순환시키면 루프로 빠져들어 로봇이나 서버의 한계를 넘을 때까지 반복시킨다.

### 9.1.9 동적 가상 웹 공간

몇몇 사이트는 그저 악의적인 애플리케이션으로 존재하며, 로봇을 함정에 빠트리기 위해 존재한다. 게이트웨이는 각각이 다음 페이지를 가리키는 가짜 웹페이지를 동적으로 무한히 생성하는 예시가 있다.

### 9.1.10 루프와 중복 피하기

웹에서 로봇이 더 올바르게 동작하기 위한 기법들

- **URL 정규화**
  - URL을 표준 형태로 변환함으로써, 같은 리소스를 가리키는 중복 URL이 생기는 것을 일부 회피
- **너비 우선 크롤링**
  - URL들을 전체에 걸쳐 너비 우선으로 스케줄링하면, 순한의 영향을 최소화할 수 있음
  - 깊이 우선 방식으로 운영하면 순환을 건드리는 순간 영원히 다른 사이트로 빠져나올 수 없음..
- **스로틀링**
  - 웹사이트에서 일정 시간동안 가져올 수 있는 페이지의 숫자를 제한한다.
- **URL 크기 제한**
  - 일정길이(보통 1kb)를 넘는 URL의 크롤링을 거부할 수 있음. 순환으로 인해 URL이 계속해서 길어진다면, 길이 제한으로 순환을 중단하게된다.
  - 이 기법을 적용하면 가져오는 콘텐츠들도 많이 생기게될 수 도 있다. 하지만 특정 크기에 도달할 때마다 에러로그를 남김으로 써 특정 사이트에서 어떤일이 벌어지는지 감시하는 사용자에겐 신호를 제공할 수 있다.
- **URL/사이트 블랙리스트**
  - 로봇 순환을 만들어 내거나 함정인것으로 알려진 사이트와 URL 목록을 만들어 관리한다.
  - 대부분의 대규모 크롤러들은 블랙리스트를 갖고있고, 이는 크롤링 되는것을 싫어하는 특정 사이트를 피하기 위해 사용된다.
- **패턴 발견**
  - 반복되는 구성요소를 가진 URL을 잠재적 순환으로 보고, 둘 이나 셋 이상 반복되는 구성요소를 갖고 있으면 해당 URL 크롤링을 거절한다.
- **콘텐츠 지문**
  - 페이지의 콘텐츠에서 몇 바이트를 얻어내어 체크섬(checksum - 중복검사)을 계산한다.
  - 만약 로봇이 이전에 보았던 체크섬을 가진 페이지를 가져오면, 그 페이지는 크롤링을 하지 않는다.
  - 체크섬 함수는, 어떤 두 페이지가 서로 내용이 다름에도 체크섬은 똑같은 확률이 적은 것을 사용해야 한다. 지문 생성용으로는 MD5와 같은 메세지 요약함수가 인기가 있다.
- **사람의 모니터링**
  - 모든 상용 수준의 로봇은 사람이 쉽게 로봇의 진행상황을 모니터링 할 수 있도록 진단과 로깅을 포함하도록 설계해야한다.

## 9.2 로봇의 HTTP

로봇도 다른 클라이언트 프로그램과 같기 때문에 HTTP 명세 규칙을 지켜야한다.

HTTP 요청을 만들고 스스로 HTTP/1.1 클라이언트라고 광고하는 로봇은 적절한 HTTP 요청 헤더를 사용해야 한다.

### 9.2.1 요청 헤더 식별하기

로봇들이 HTTP를 최소한도로만 지원하려고 해도 신원 식별 헤더(특히 User-Agent HTTP헤더)를 구현하고 전송한다. 로봇 구현자들은 기본적인 몇가지 헤더를 사이트에게 보내주는것이 좋다.

- User-Agent: 서버에게 요청을 만든 로봇의 이름을 말해준다.
- From: 로봇의 사용자/관리자의 이메일 주소를 제공한다.
- Accept: 서버에게 어떤 미디어 타입을 보내도 되는지 말해준다.
- Refer: 현재의 요청 URL을 포함한 문서의 URL을 제공한다.

### 9.2.2 가상 호스팅

요청에 Host 헤더를 포함하지 않으면 로봇이 어떤 URL에 대해 잘못된 콘텐츠를 찾게 만든다. 이러한 이유로 HTTP/1.1은 Host헤더를 사용할 것을 요구한다.

### 9.2.3 조건부 요청

때때로 로봇은 방대한 양의 요청을 시도하기 때문에 시간이나 엔터티 태그를 비교함으로써 업데이트 된 내용이 있는지 알아보는 조건부 HTTP요청을 구현한다.

이는 HTTP캐시가 전에 받아온 리소스의 로컬 사본의 유효성을 검사하는 방법과 매우 비슷하다.

### 9.2.4 응답 다루기

몇몇 로봇들은 웹 탐색이나 서버와의 상호작용을 잘해보려한다. 이를 하기 위해선 HTTP응답을 다룰 줄 알 필요가 있다

- 상태코드
  - 모든 로봇은 200 OK이나 404 Not Found 와 같은 HTTP 상태코드를 이해해야한다.
- 엔터티
  - HTTP 헤더에 임베딩된 정보를 따라 로봇들은 엔터티 자체의 정보를 찾을 수 있다.
  - http-equiv 태그와 같은 메타 HTML태그는 리소스에 대해 콘텐츠 저자가 포함시킨 정보이다. 로봇 구현자들은 http-equiv 정보를 찾아내기 위해 HTML문서의 HEAD 태그를 탐색하길 원할 수도 있다.

### 9.2.5 User-Agent 타기팅

사이트 관리자들은 로봇의 요청을 다루기 위한 전략을 세워야한다.

## 9.3 부적절하게 동작하는 로봇들

- **폭주하는 로봇**
  - 로봇은 웹 서핑을 하는 사람보다 HTTP 요청을 훨씬 빠르게 만들 수 있기 때문에 에러를 갖고있거나, 순환에 빠졌다면 웹서버에 극심한 부하를 안겨줄 수 있어 로봇 저자들은 이를 방지할 보호장치를 마련해야한다.
- **오래된 URL**
  - 몇몇 로봇은 오래된 URL을 방문할 수 있는데, 이는 존재하지 않는 문서에 대한 접근 요청으로 에러 로그가 채워지거나, 에러 페이지를 제공하는 서버의 부하로 인해 웹 사이트 관리자를 짜증나게 한다.
- **길고 잘못된 URL**
  - 웹사이트에게 크고 의미없는 URL을 요청할 수 있는데, 그 URL이 충분히 길다면 웹서버의 처리 능력에 영향을 주고, 로그를 어지럽게 채우는 등 고장을 일으키는 원인이 될 수 있다.
- **호기심이 지나친 로봇**
  - 사적인 데이터에 대한 URL을 얻어 그 데이터를 인터넷 검색엔진이나 기타 애플리케이션을 통해 쉽게 접근하게 만들 수 있는데, 이것은 민감한 데이터를 검색할 수 있기 때문에 이런 컨텐츠를 무시하게 하는 매커니즘이 중요하다.
- **동적 게이트웨이 접근**
  - 로봇은 게이트웨이 어플리케이션의 콘텐츠에 대한 URL로 요청을 할 수도 있는데 이는 데이터 처리비용이 들기 때문에, 웹 사이트 관리자들이 좋아하지 않는다.

## 9.4 로봇 차단하기

로봇의 접근을 제어하는 정보를 저장하는 파일을 robots.txt라고 한다. 이는 웹서버 문서 루트에 robots.txt라고 이름이 붙은 선택적인 파일을 제공할 수 있다. 이 파일은 어떤 로봇이 서버의 어떤 부분에 접근할 수 있는지에 대한 정보가 담겨있다. 만약 어떤 로봇이 이 자발적인 표준에 따른다면 그것은 웹사이트의 다른 리소스들에 접근하기 전에 이 사이트의 robots.txt 파일을 요청할 것이다.

- 웹로봇 클라이언트 → robots.txt 요청 → 접근허용확인? ok → /speacials/acetylenet.html 접근

### 9.4.1 로봇 차단 표준

대부분의 주류 업체들과 검색엔진 크롤러들은 이 차단 표준을 지원한다.

오늘날 대부분의 로본들은 0.0이나 1.0 표준을 채택했다.

| 버전 | 이름과 설명                                                                                     | 날짜        |
| ---- | ----------------------------------------------------------------------------------------------- | ----------- |
| 0.0  | 로봇 배제 표준-Disallow 지시자를 지원하는 마틴 코스터의 오리지널 robots.txt 메커니즘            | 1993년 6월  |
| 1.0  | 웹 로봇 제어 방법-Allow 지시자의 지원이 추가된 마틴 코스터의 IETF 초안                          | 1996년 11월 |
| 2.0  | 로봇 차단을 위한 확장 표준-정규식과 타이밍 정보를 포함한 숀 코너의 확장. 널리 지원되지는 않는다 | 1996년 11월 |

### 9.4.2 웹 사이트와 robots.txt 파일들

웹 사이트의 어떤 URL을 방문하기 전에 그 웹사이트에 robots.txt 파일이 존재한다면 로봇은 반드시 그 파일을 가져와 처리해야한다.

- **robots.txt 가져오기**
  - 로봇은 웹 서버의 여느 파일들과 마찬가지로 HTTP GET 메서드를 통해 robots.txt 리소스를 가져온다.
    robots.txt가 존재한다면 서버는 그 파일을 text/plain 본문으로 반환한다.
  - Not Found로 응답한다면 로봇은 그 서버에 접근제한을 하지 않는것으로 받아들여 어떤 파일이든 요청하게 된다.
  - 상용 웹 로봇이 보낼 수 있는 HTTP 크롤러 요청의 예 (헤더를 통해 신원 정보를 넘김으로써 로봇의 접근을 추척할 수 있도록 해야한다)
    ```markdown
    GET /robots.txt HTTP/1.0\
    Host: www.joes-hardware.com
    User-Agent: Slurp/2.0
    Date: Wed Oct 3 20:22:48 EST 2001
    ```
- **응답코드**
  - **`2XX`:** 성공으로 응답하면 반드시 그 응답 컨텐츠를 파싱하여 차단 규칙을 얻고, 그 규칙에 따라야한다.
  - **`404`** : 차단 규칙이 존재 하지 않는다고 가정하고 robots.txt의 제약 없이 사이트에 접근한다.
  - **`401`, `403`** (접근제한): 그사이트로 접근은 완전히 제한되어있다고 가정해야함.
  - **`503`** (일시적 실패): 그 사이트 리소스를 검색하는걸 뒤로 미뤄야한다
  - **`3XX`** (리다이렉션)**:** 리소스가 발견될 때까지 리다이렉트를 따라가야한다.

### 9.4.3 robots.txt 파일 포맷

robots.txt파일의 각 줄은 빈줄, 주석줄, 규칙줄 세가지 종류가 있다.

규칙줄은 HTTP헤더처럼 생겼고, <필드>:<값> 패턴 매칭을 위해 사용된다.

```bash
# 이 robots.txt 파일은 Slurp과 Webcrawler가 우리 사이트의 공개된
# 영역을 크롤링하는 것을 허락한다. 그러나 다른 로봇은 안된다..

User-Agent: slurp
User-Agent: webcrawler
Disallow: /private

User-Agent: *
Disalow:
```

이 줄들은 레코드로 구분되고, 각 레코드는 규칙 줄들의 집합으로 되어있으며 빈 줄이나 파일의 끝(end-of-file)문자로 끝낸다. 어떤 로봇이 이 레코드에 영향을 받는지 지정하는 하나 이상의 User-Agent줄로 시작하며 로봇들이 접근할 수 있는 URL들을 말해주는 Allow 줄과 Disallow줄이 온다.

- **User-Agent 줄**
  - `User-Agent: <robot-name>`
  - `User-Agent: *`
- **Disallow, Allow**
  - 로봇 차단 레코드 User-Agent 줄들 바로 다음에 온다. 특정 로봇에 대해 어떤 URL 경로가 명시적으로 금지/허용 되는지 기술한다.
    - `Disallow: /tmp` 에 매칭되는 URL들
      - http://www.joes-hardware.com/tmp
      - http://www.joes-hardware.com/tmp/
      - http://www.joes-hardware.com/tmp/pliers.html
      - http://www.joes-hardware.com/tmpspc/stufff.txt
- **Disallow/Allow 접두매칭(prefix matching)**
  - Disallow나 Allow 규칙이 어떤 경로에 적용되려면, 그 경로의 시작으로부터 규칙 경로의 길이만큼 문자열이 규칙 경로와 같아야한다.(대소문자의 차이도 없어야한다.)
    User-Agent 줄과 달리 \*는 특별한 의미를 갖지 않지만, 대신 빈 문자열을 이용해 모든 문자열에 매치되게 할 수 있다.
  - 규칙 경로나 URL 경로의 임의의 ‘이스케이핑’된 문자들(%XX)은 비교전에 원래대로 복원된다.
  - 어떤 규칙 경로가 빈 문자열이면, 그 규칙은 모든 URl 경로와 매치된다.

### 9.4.4 그 외에 알아둘 점

- User-Agent, Disallow, Allow외에 다른 필드를 포함할 수 있으며, 로봇은 자신이 이해하지 못하는 필드는 무시해야한다.
- 하위 호환성을 위해, 한줄을 여러줄로 나누어 적는 것은 허용되지 않는다.
- 주석은 파일의 어디든 허용되고, 주석은 선택적인 공백문자와 뒤이은 주석문자(#)로 시작해서 그 뒤에 줄바꿈 문자가 나올때까지 주석내용으로 간주한다.
- 로봇 차단 표준버전 0.0은 Allow 줄을 지원하지 않는다.

### 9.4.5 robots.txt 캐싱과 만료

로봇은 주기적으로 robots.txt 파일을 가져와서 그 결과를 캐시해야 한다. robots.txt의 캐시된 사본은 robots.txt 파일이 만료될 때 까지 로봇에 의해 사용된다.

파일의 캐싱 제어를 위해 HTTP응답의 Cache-Control과 Expires 헤더에 주의를 기울여야한다.

요즘의 크롤러 제품들은 HTTP/1.1 클라이언트가 아니다. 웹 마스터들은 이 크롤러들이 robots.txt 리소스에 적용되는 캐시 지시자를 이해하지 못할 수도 있다는것을 주의해야한다.

### 9.4.6 로봇 차단 펄 코드

robots.txt 파일과 상호작용하는 공개된 펄(Perl)라이브러리가 몇가지 존재하는데. 한 예는 CPAN공개 펄 아카이브의 `WWW::RobotRules` 모듈이다.

WWW::RobotRules API의 주요 메서드

- **RobotRules 객체 만들기**
  - `$rules = WWW::RobotRules→new($robot_name);`
- **robots.txt 파일 로드하기**
  - `$rules→parse($url, $content, $fresh_until);`
- **사이트 URL을 가져올 수 있는지 검사하기**
  - `$can_fetch = $rules→allowed($url)`

### 9.4.7 HTML 로봇 제어 META 태그

robots.txt 파일을 통해 로봇을 제한하는것은 웹 관리자만이 할 수 있기 때문에 페이지별로 제한하는것은 HTML 페이지 저자들이 HTML의 META 태그를 통해 구현한다.

`<META NAME=”ROBOTS” CONTENT=”directive-list”>`

- **가장 널리 쓰이는 로봇 META 지시자**
  - NOINDEX: 로봇에게 이 페이지를 처리하지 말고 무시하라고 말한다.
    - `<META NAME=”ROBOTS” CONTENT=”NOINDEX”>`
  - NOFOLLOW: 로봇에게 이 페이지가 링크한 페이지를 크롤링 하지 말라고 말해준다
    - `<META NAME=”ROBOTS” CONTENT=”NOFOLLOW”>`
- INDEX: 로봇에게 이 페이지의 콘텐츠를 인덱싱해도 된다고 말해준다.
- FOLLOW: 로봇에게 이 페이지가 링크한 페이지를 크롤링해도 된다고 말해준다
- NOARCHIVE: 로봇에게 이 페이지의 캐시를 위한 로컬 사본을 만들어서는 안된다고 말해준다.
- ALL: INDEX, FOLLOW와 같다
- NONE: NOINDEX, NOFOLOW와 같다.

지시자들이 서로 충돌하거나 중복되게 해서는 안된다!

`<meta name=”robots” content=”INDEX,NOINDEX,NOFOLLOW”>`

- **검색엔진 META 태그**
  - 모든 로봇 META 태그는 `name=”robots”` 속성을 포함한다.
  - DESCRIPTION, KEY WORDS META 태그는 콘텐츠의 색인을 만드는 검색엔진 로봇들에대해 유용하다.

| name=       | content= | 설명                                                 |
| ----------- | -------- | ---------------------------------------------------- |
| DESCRIPTION | <텍스트> | 저자가 웹페이지의 짧은 요약을 정의할 수 있게 해준다. |

많은 검색엔진이 페이지 저자가 그들의 웹 페이지에 대해 기술하는 요약을 지정할 수 있게 해준다.

<meta name=”description” content=”메리의 골동품 상점에 오신것을 환영합니다”> |
| KEYWORDS | <쉼표 목록> | 키워드 검색을 돕기위한, 웹페이지를 기술하는 단어들의 쉼표로 구분되는 목록 |
| REVISIT-AFTER | <숫자 days> | 로봇이나 검색엔진에게, 이 페이지는 아마도 쉽게 변경될 것이기때문에 지정된 만큼의 날짜가 지난 이후에 다시 방문해야한다고 지시한다. |

## 9.5 로봇 에티켓

웹 로봇 커뮤니티의 개척자인 마틴 코스터는 웹 로봇을 만드는 사람들을 위한 가이드라인 목록을 작성했는데, 그 상세 가이드라인은 아래 링크에서 확인할 수 있다.

[http://www.robotstxt.org/](http://www.robotstxt.org/wc/guidlines.html)

robots.txt 파일에 대해서 찾아보다 구글에서도 가이드가 써져있는것 발견함

https://developers.google.com/search/docs/crawling-indexing/robots/intro?hl=ko

## 9.6 검색 엔진

웹 로봇을 가장 광범위하게 사용하는 것은 인터넷 검색엔진이다.

웹 크롤러들은 검색엔진에게 웹에 존재하는 문서들을 가져다주고, 검색엔진이 어떤 단어들이 존재하는지 색인을 생성할 수 있게 해준다.

### 9.6.1 넓게 생각하라

웹의 초창기 시절엔 검색엔진들은 사용자들이 웹 상의 문서 위치를 알아내는것 정도만 도와주는 단순한 데이터베이스였다. 현재는 수십억게의 페이지에 접근이 가능해지면서 수십억개의 웹 페이지에서 원하는 정보를 찾기 위한 크복잡한 크롤러를 사용해야한다. 규모가 매우 크기 때문에 웹 전체를 크롤링 하는것은 쉽지 않다.

### 9.6.2 현대적인 검색엔진 아키텍처

오늘날 검색 엔진들은 그들이 갖고있는 전 세계의 웹페이지들에 대해 ‘풀 텍스트 색인(full-text-indexes)’이라고 하는 복잡한 로컬 데이터베이스를 생성한다. 이 색인은 웹으

- `웹 검색 사용자`→`질의` → `[웹 검색 게이트웨이]` ↔ `[풀 텍스트 색인 DB]`←`[검색엔진 크롤러/색인기]`←`웹서버`

### 9.6.3 풀 텍스트 색인

풀 텍스트 색인은 단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터베이스 이다.

이 문서들은 색인이 생성된 후엔 검색할 필요가 없다. 풀 텍스트 색인은 각 단어를 포함한 문서들을 열거한다.

### 9.6.4 질의 보내기

사용자가 질의를 웹 검색엔진 게이트웨이로 보내는 방법은, HTML 폼을 사용자가 채워넣고 브라우저가 그 폼을 HTTP GET이나 POST 요청을 이용해서 게이트웨이로 보내는 식이다.

게이트웨이 프로그램은 검색 질의를 추출하고 웹 UI 질의를 풀 텍스트 색인을 검색할 때 사용되는 표현식으로 반홚나다.

### 9.6.5 검색 결과를 정렬하고 보여주기

질의의 결과를 확인하기 위해 검색엔진이 색인을 한번 사용했다면, 게이트웨이 어플리케이션은 그 결과를 이용해 최종 사용자를 위한 결과 페이지를 즉석에서 만들어낸다.

검색엔진은 결과에 순위를 매기기위해 똑똑한 알고리즘을 사용한다. 이는 관련도 랭킹(relevancy ranking)이라고 불리며, 검색 결과의 목록에 점수를 매기고 정렬하는 과정이다.

### 9.6.6 스푸핑

사용자들은 자신이 검색한 결과에 대해 찾는 내용이 최상위 몇줄내 보이지 않으면 불만족스러워 한다. 이를 통해 알 수 있듯이 검색 순위는 매우 중요하다.

검색 결과에 상위 순위를 차지하기 위해 가짜 페이지를 만들거나, 연관 없는 내용으로 알고리즘을 속이게 만드는 속임수를 알아차릴 수 있도록 알고리즘을 잘 수정해야한다.
